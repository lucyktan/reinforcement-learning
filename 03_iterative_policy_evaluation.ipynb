{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Name: Lucy Tan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Environment below has three states: 1, 2, and 3. Possible transitions are: (1) 1->1, 1->2; (2) 2->1, 2->2, 2->3; and (3) 3->2, 3->3.\n",
    "\n",
    "Actions of the Agent are decoded by -1, 0, and +1, which correspond to its intention to move left, stay, and move right, respectively. The Environment, however, does not always respond to these intentions exactly, and there is 10% chance that action 0 will result in moving to the left (if moving to the left is admissible), and 1 action will result in staying - in other words, there is an \"east wind\" (please see get_reward() of the Environment). \n",
    "\n",
    "Further, we assume that the number of steps, T, is infinite and whenever the process enters state 3, the Environment generates reward = 1. In all other cases the reward is 0. For example, transition 2->3 will result in reward 1, transition 3->3 will result in reward 1, transition 3->2 will result in reward 0, transition 2->2 will result in reward 0, etc.\n",
    "\n",
    "Let’s use $\\gamma=0.9$. Currently, the Agent always selects action 0 (has an intention to stay). Please notice that without the wind, the state-values would be [0,0,1/0.1] for this policy. With the wind, the value of state 3, however, becomes only 4.74 because the transition 3->2 happens sooner or later with probability 1. The Agent does not make an attempt to return to state 3 – its intention is to stay.\n",
    "\n",
    "Function reward_cumulative(T=10, S0=1, gamma=1) returns the observed cumulative discounted reward for T number of steps if the process starts in state S0. Please notice that given $\\gamma=0.9$, T=100 does not make this estimate much different from infinite time because $\\gamma^T$ is of order $10^{-5}$.\n",
    "\n",
    "Function V_estimate(T=10, S0=1, gamma=0.9, n_trials=10) calls reward_cumulative() function n_trials number of times and then estimates the state-value based on these n_trials paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, S0 = 1):\n",
    "        self.time = 0\n",
    "        self.state = S0\n",
    "\n",
    "    def admissible_actions(self):\n",
    "        A = list((-1,0,1))\n",
    "        if self.state == 1: A.remove(-1)\n",
    "        if self.state == 3: A.remove(1)\n",
    "        return A\n",
    "    \n",
    "    def check_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        self.time += 1\n",
    "        move = action\n",
    "        if self.state > 1 and move > -1:\n",
    "            move = np.random.choice([move-1, move],p=[0.1,0.9])\n",
    "        self.state += move\n",
    "        if self.state == 3:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.current_reward = 0.0\n",
    "\n",
    "    def step(self, env):\n",
    "        #actions = env.admissible_actions()\n",
    "        action_selected = 0\n",
    "        reward = env.get_reward(action_selected)            \n",
    "        self.current_reward = reward\n",
    "        \n",
    "def reward_cumulative(T=10, S0=1, gamma=1):\n",
    "    env = Environment(S0)\n",
    "    agent = Agent()\n",
    "    G = 0\n",
    "    while env.time <= T:\n",
    "        agent.step(env)\n",
    "        G += gamma**(env.time-1)*agent.current_reward\n",
    "    return G\n",
    "\n",
    "def V_estimate(T=10, S0=1, gamma=0.9, n_trials=10):\n",
    "    V_estimate = 0\n",
    "    for i in range(1,n_trials+1):\n",
    "        #V_estimate = (V_estimate*(i-1) + reward_cumulative(T, S0, gamma))/i\n",
    "        V_estimate = V_estimate+(reward_cumulative(T, S0, gamma)-V_estimate)/i \n",
    "    return V_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state-value function:\n",
      "[0.  0.  4.7]\n"
     ]
    }
   ],
   "source": [
    "T = 100\n",
    "V = np.array([V_estimate(T, S0 = s, gamma=0.9, n_trials=10000) for s in range(1,4)])\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print(\"state-value function:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 1 (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please add state 4 and 5 to the Environment. For entering states 4 and 5 assume no rewards. Please keep current reward from entering state 3 as is, i.e. 2->3, 3->3, 4->3 will result in reward=1. All other cases correspond to 0 reward.\n",
    "\n",
    "For current actions of the Agent, keep $\\gamma=0.9$ and estimate the state-values using V_estimate() function with T = 100 and n_trials=10000. Print the result for states 1, 2, 3, 4, 5. What value of state 4 do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state-value function:\n",
      "[0.   0.   4.76 2.75 1.3 ]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, S0 = 1):\n",
    "        self.time = 0\n",
    "        self.state = S0\n",
    "\n",
    "    def admissible_actions(self):\n",
    "        A = list((-1,0,1))\n",
    "        if self.state == 1: A.remove(-1)\n",
    "        if self.state == 5: A.remove(1)\n",
    "        return A\n",
    "    \n",
    "    def check_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        self.time += 1\n",
    "        move = action\n",
    "        if self.state > 1 and move > -1:\n",
    "            move = np.random.choice([move-1, move],p=[0.1,0.9])\n",
    "        self.state += move\n",
    "        if self.state == 3:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.current_reward = 0.0\n",
    "\n",
    "    def step(self, env):\n",
    "#         actions = env.admissible_actions()\n",
    "        action_selected = 0\n",
    "        reward = env.get_reward(action_selected)            \n",
    "        self.current_reward = reward\n",
    "        \n",
    "def reward_cumulative(T=10, S0=1, gamma=1):\n",
    "    env = Environment(S0)\n",
    "    agent = Agent()\n",
    "    G = 0\n",
    "    while env.time <= T:\n",
    "        agent.step(env)\n",
    "        G += gamma**(env.time-1)*agent.current_reward\n",
    "    return G\n",
    "\n",
    "def V_estimate(T=10, S0=1, gamma=0.9, n_trials=10):\n",
    "    V_estimate = 0\n",
    "    for i in range(1,n_trials+1):\n",
    "        #V_estimate = (V_estimate*(i-1) + reward_cumulative(T, S0, gamma))/i\n",
    "        V_estimate = V_estimate+(reward_cumulative(T, S0, gamma)-V_estimate)/i \n",
    "    return V_estimate\n",
    "\n",
    "T = 100\n",
    "V = np.array([V_estimate(T, S0 = s, gamma=0.9, n_trials=10000) for s in range(1,6)])\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print(\"state-value function:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Environment you developed in Problem 1, change the actions of the Agent to the optimal, that is, from states 1 and 2 it will want to move to right, stay in stay 3, and move to left from states 4 and 5.\n",
    "\n",
    "For these actions of the Agent and $\\gamma=0.9$, estimate the state-values using V_estimate() function with T = 100 and n_trials=10000. Print the result for states 1, 2, 3, 4, 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state-value function:\n",
      "[8.012059 8.986328 8.991496 9.114012 8.18935 ]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, S0 = 1):\n",
    "        self.time = 0\n",
    "        self.state = S0\n",
    "\n",
    "    def admissible_actions(self):\n",
    "        A = list((-1,0,1))\n",
    "        if self.state == 1: A.remove(-1)\n",
    "        if self.state == 5: A.remove(1)\n",
    "        return A\n",
    "    \n",
    "    def check_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        self.time += 1\n",
    "        move = action\n",
    "        if (self.state == 1 and move > 0) or (self.state > 1 and move > -1):\n",
    "            move = np.random.choice([move-1, move],p=[0.1,0.9])\n",
    "        self.state += move\n",
    "        if self.state == 3:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.current_reward = 0.0\n",
    "\n",
    "    def step(self, env):\n",
    "        actions = env.admissible_actions()\n",
    "        action_selected = self.choose_action(actions, env)\n",
    "        reward = env.get_reward(action_selected)            \n",
    "        self.current_reward = reward\n",
    "    \n",
    "    def choose_action(self, actions, env):\n",
    "        if env.check_state() < 3 and 1 in actions:\n",
    "            chosen_action = 1\n",
    "        elif env.check_state() > 3 and -1 in actions:\n",
    "            chosen_action = -1\n",
    "        else:\n",
    "            chosen_action = 0\n",
    "        return chosen_action\n",
    "        \n",
    "def reward_cumulative(T=10, S0=1, gamma=1):\n",
    "    env = Environment(S0)\n",
    "    agent = Agent()\n",
    "    G = 0\n",
    "    while env.time <= T:\n",
    "        agent.step(env)\n",
    "        G += gamma**(env.time-1)*agent.current_reward\n",
    "    return G\n",
    "\n",
    "def V_estimate(T=10, S0=1, gamma=0.9, n_trials=10):\n",
    "    V_estimate = 0\n",
    "    for i in range(1,n_trials+1):\n",
    "        #V_estimate = (V_estimate*(i-1) + reward_cumulative(T, S0, gamma))/i\n",
    "        V_estimate = V_estimate+(reward_cumulative(T, S0, gamma)-V_estimate)/i \n",
    "    return V_estimate\n",
    "\n",
    "T = 100\n",
    "V = np.array([V_estimate(T, S0 = s, gamma=0.9, n_trials=10000) for s in range(1,6)])\n",
    "\n",
    "np.set_printoptions(precision=6)\n",
    "print(\"state-value function:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (15 points)\n",
    "For the Environment and Agent you developed in Problem 2, please obtain the state-value for this policy in an alternative way without simulations by solving the Bellman equation (eq. 4.5) numerically: use the iterative policy evaluation algorithm on p.75 of \"Reinforcement Learning\" by Sutton and Barto.\n",
    "\n",
    "Please notice that for these actions the policy $\\pi(a|s)$ is<br>\n",
    "$\\pi(-1|1)=0, \\pi(0|1)=0, \\pi(+1|1)=1$,<br>\n",
    "$\\pi(-1|2)=0, \\pi(0|2)=0, \\pi(+1|2)=1$,<br>\n",
    "$\\pi(-1|3)=0, \\pi(0|3)=1, \\pi(+1|3)=0$,<br>\n",
    "$\\pi(-1|4)=1, \\pi(0|4)=0, \\pi(+1|4)=0$,<br>\n",
    "etc.\n",
    "\n",
    "\n",
    "The non-zero transition probabilities $p(s^\\prime,r|s,a)$ are<br>\n",
    "\n",
    "$p(s^\\prime=1,r=0|s=1,a=0)=1$,<br>\n",
    "$p(s^\\prime=1,r=0|s=1,a=+1)=0.1,p(s^\\prime=2,r=0|s=1,a=+1)=0.9$,<br>\n",
    "\n",
    "$p(s^\\prime=1,r=0|s=2,a=-1)=1$,<br>\n",
    "$p(s^\\prime=1,r=0|s=2,a=0)=0.1,p(s^\\prime=2,r=0|s=2,a=0)=0.9$,<br>\n",
    "$p(s^\\prime=2,r=0|s=2,a=+1)=0.1,p(s^\\prime=3,r=1|s=2,a=+1)=0.9$,<br>\n",
    "\n",
    "$p(s^\\prime=2,r=0|s=3,a=-1)=1$,<br>\n",
    "$p(s^\\prime=2,r=0|s=3,a=0)=0.1,p(s^\\prime=3,r=1|s=3,a=0)=0.9$,<br>\n",
    "$p(s^\\prime=3,r=1|s=3,a=+1)=0.1,p(s^\\prime=4,r=0|s=3,a=+1)=0.9$,<br>\n",
    "\n",
    "etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state-value function:\n",
      "{1: 8.010180935001125, 2: 8.999200812758113, 3: 8.999209603728897, 4: 9.099288643356008, 5: 8.189359779020407}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "class Environment:\n",
    "    def admissible_actions(self, state):\n",
    "        A = list((-1,0,1))\n",
    "        if state == 1: A.remove(-1)\n",
    "        if state == 5: A.remove(1)\n",
    "        return A\n",
    "    \n",
    "    def admissible_states(self):\n",
    "        return list(range(1,6))\n",
    "    \n",
    "    def get_trans_probs(self, action, state):\n",
    "        mapping = {}\n",
    "        new_state_no_wind = state + action\n",
    "        reward_no_wind = 1 if new_state_no_wind == 3 else 0       \n",
    "        if (state == 1 and action > 0) or (state > 1 and action > -1):\n",
    "            mapping[(new_state_no_wind, reward_no_wind)] = 0.9\n",
    "            new_state_wind = new_state_no_wind - 1\n",
    "            reward_wind = 1 if new_state_wind == 3 else 0\n",
    "            mapping[(new_state_wind, reward_wind)] = 0.1\n",
    "        else:\n",
    "            mapping[(new_state_no_wind, reward_no_wind)] = 1.0\n",
    "        return mapping\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def get_policy(self, action, state):\n",
    "        policy = 0\n",
    "        if (state < 3 and action == 1) or (state > 3 and action == -1) or (state == 3 and action == 0):\n",
    "            policy = 1\n",
    "        return policy\n",
    "    \n",
    "    \n",
    "def v_estimate(env, agent, V, gamma, theta):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in env.admissible_states():\n",
    "            v = V[state]\n",
    "            v_sum = 0\n",
    "            for action in env.admissible_actions(state):\n",
    "                action_prob = agent.get_policy(action, state)\n",
    "                transition_probs = env.get_trans_probs(action, state)\n",
    "                for new_state, reward in transition_probs:\n",
    "                    trans_prob = transition_probs.get((new_state, reward))\n",
    "                    v_new_state = V[new_state]\n",
    "                    v_sum += action_prob * trans_prob * (reward + gamma * v_new_state)\n",
    "\n",
    "            V[state] = v_sum\n",
    "            delta = max(delta, abs(v - V[state]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "V = collections.defaultdict(int)  # defaults to zero\n",
    "gamma = 0.9\n",
    "theta = 0.0001\n",
    "print(\"state-value function:\")\n",
    "print(dict(v_estimate(env, agent, V, gamma, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
