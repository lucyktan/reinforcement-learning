{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Name: Lucy Tan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antiviral drugs are used for treating viral infections. The procedure often requires a sequence of treatments. Over time patients develop resistance to the drugs and need to keep switching them. Moreover, a drug of type 1 can make the patient (partially) resistant to the drug of type 2.\n",
    "\n",
    "In this problem we consider drugs of type A, B, and C, all of which can potentially lower the viral load but the effect depends on their order. The 'Viral_load.xlsx' dataset provides results for 1,000 of patients, where rewards (based on the observed viral load level of patients) indicate the effectiveness of the preceding drugs used. For example, the first patient used drugs A, B, and C in this order and received the following rewards: 14.9062033, 13.67860579, and 12.76096513, respectively.\n",
    "\n",
    "Because of the antiviral drug resistance, same drug cannot be used twice. The policy (behavior policy $b$) that was used for treatments represented in the 'Viral_load.xlsx' dataset is\n",
    "\n",
    "1) if all drugs are available, pick A with probability 0.7, B with probability 0.2, and C with probability 0.1   \n",
    "2) if all drugs, but A, are available, pick B with probability 0.65 and C with probability 0.35   \n",
    "3) if all drugs, but B, are available, pick A with probability 0.9 and C with probability 0.1  \n",
    "4) if all drugs, but C, are available, pick A with probability 0.8 and B with probability 0.2   \n",
    "5) if only one drug is available, pick it with probability 1    \n",
    "\n",
    "\n",
    "\n",
    "One can find that based on these observations, the optimal order of the treatment is B, A, and then C, indicating that drug A diminishes the effect of drug B, and C diminishes the effect of drug A (and possibly B).\n",
    "\n",
    "The sequence of treatments can be represented as a Markov Decision Process (MDP). Clearly, the set of available actions at time t depends on the intire history. In order to model the treatment proceedure with the MDP, we introduce states as follows:\n",
    "\n",
    "1) $S_{ABC}=${all drugs are available}   \n",
    "2) $S_{BC}=${all drugs, except A, are available}   \n",
    "3) $S_{AC}=${all drugs, except B, are available}       \n",
    "4) $S_{AB}=${all drugs, except C, are available}   \n",
    "5) $S_{A}=${only A is available}   \n",
    "6) $S_{B}=${only B is available}   \n",
    "7) $S_{C}=${only C is available}      \n",
    "\n",
    "The optimal policy $\\pi$, that corresponds to the sequence B, A, C, is then: (1) select B in state $S_{ABC}$; (2) select A in state $S_{AC}$; (3) select C in state $S_{C}$. The other actions do not need to be specified because the MDP will never reach them.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, create a function that returns the importance-sampling ratio for each sequence of type \"ABC\" (use input format of choice), given target and behavior policies, $\\pi$ and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('A', 'B', 'C'): 0.0, ('A', 'C', 'B'): 0.0, ('B', 'A', 'C'): 5.555555555555555, ('B', 'C', 'A'): 0.0, ('C', 'A', 'B'): 0.0, ('C', 'B', 'A'): 0.0}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def admissible_states():\n",
    "    # [(0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)]\n",
    "    # where (0, 0, 1) means drug C is available\n",
    "    states = list(itertools.product(*[[0, 1]] * 3))\n",
    "    states.remove((0,0,0))\n",
    "    return states\n",
    "\n",
    "def action_index(action):\n",
    "    location = {\n",
    "        'A': 0,\n",
    "        'B': 1,\n",
    "        'C': 2\n",
    "    }\n",
    "    return location.get(action, None)\n",
    "\n",
    "def is_admissible_action(action, state):\n",
    "    # if state is (0, 0, 1), admissible actions are (0, 0, 1), meaning giving drug C is an admissible action\n",
    "    return bool(state[action_index(action)])\n",
    "\n",
    "def get_behavior_policy(state):\n",
    "    if sum(state) == 1:\n",
    "        policy_probs = state\n",
    "    elif sum(state) == 2:\n",
    "        probs_2 = {\n",
    "            (1, 1, 0): (0.8, 0.2, None),\n",
    "            (1, 0, 1): (0.9, None, 0.1),\n",
    "            (0, 1, 1): (None, 0.65, 0.35)\n",
    "        }\n",
    "        policy_probs = probs_2.get(state)\n",
    "    elif sum(state) == 3:\n",
    "        policy_probs = (0.7, 0.2, 0.1)\n",
    "    return policy_probs\n",
    "\n",
    "def get_target_policy(state):\n",
    "    probs = {\n",
    "        (1, 1, 1): (0, 1, 0),\n",
    "        (1, 1, 0): (0.5, 0.5, None),\n",
    "        (1, 0, 1): (1, None, 0),\n",
    "        (0, 1, 1): (None, 0.5, 0.5),\n",
    "        (1, 0, 0): (1, None, None),\n",
    "        (0, 1, 0): (None, 1, None),\n",
    "        (0, 0, 1): (None, None, 1)\n",
    "    }\n",
    "    return probs.get(state, None)\n",
    "\n",
    "def get_behavior_prob(action, state):\n",
    "    return get_behavior_policy(state)[action_index(action)] if is_admissible_action(action, state) else None\n",
    "\n",
    "def get_target_prob(action, state):\n",
    "    return get_target_policy(state)[action_index(action)] if is_admissible_action(action, state) else None\n",
    "    \n",
    "def get_importance_sampling_ratio(sequence):\n",
    "    state = [1, 1, 1]\n",
    "    ratio = 1\n",
    "    for action in sequence:\n",
    "        target_prob = get_target_prob(action, tuple(state))\n",
    "        behavior_prob = get_behavior_prob(action, tuple(state))\n",
    "        if target_prob is not None and behavior_prob is not None:\n",
    "            ratio = ratio * target_prob / behavior_prob\n",
    "        else:\n",
    "            ratio = None\n",
    "            break\n",
    "        state[action_index(action)] = 0\n",
    "    return ratio\n",
    "    \n",
    "def sequences():\n",
    "    return list(itertools.permutations(('A', 'B', 'C')))\n",
    "    \n",
    "importance_sampling_ratios = {}\n",
    "for sequence in sequences():\n",
    "    importance_sampling_ratios[sequence] = get_importance_sampling_ratio(sequence)\n",
    "print(importance_sampling_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 'Viral_load.xlsx' dataset, estimate the state value function $v_\\pi(s)$ for $s\\in\\{s_{ABC},s_{AC},s_{C}\\}$ via Off-policy MC prediction (section 5.6 of \"Reinforcement Learning\" by Sutton and Barto). Please notice that generating an episode under policy $b$ would correspond to reading a row from 'Viral_load.xlsx'. Print the result for $\\gamma=0.99$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 1, 1): 40.76285747641467, (1, 0, 1): 27.705034462285298, (0, 0, 1): 14.025463818322676}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import csv\n",
    "\n",
    "def off_policy_mc_prediction(episodes, gamma):\n",
    "    V = collections.defaultdict(float)\n",
    "    C = collections.defaultdict(float)\n",
    "    for episode in episodes:\n",
    "        state = [0, 0, 0]\n",
    "        G = 0\n",
    "        W = 1\n",
    "        for time_step in episode[::-1]:\n",
    "            action = time_step[0]\n",
    "            reward = time_step[1]\n",
    "            state[action_index(action)] = 1\n",
    "            G = gamma*G+reward\n",
    "            C[tuple(state)] += W\n",
    "            V[tuple(state)] += (W/C[tuple(state)])*(G-V[tuple(state)])\n",
    "            W = W*(get_target_prob(action, tuple(state))/get_behavior_prob(action, tuple(state)))  \n",
    "    return V\n",
    "        \n",
    "def generate_episodes(csv_file):\n",
    "    episodes = []\n",
    "    with open(csv_file, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in list(reader)[1:]:\n",
    "            episode = [[row[1], float(row[2])],\n",
    "                       [row[3], float(row[4])],\n",
    "                       [row[5], float(row[6])]]\n",
    "            episodes.append(episode)\n",
    "    return episodes\n",
    "    \n",
    "csv_file = 'Viral_load.csv'\n",
    "episodes = generate_episodes(csv_file)\n",
    "states = [(1, 1, 1), (1, 0, 1), (0, 0, 1)]\n",
    "gamma = 0.99\n",
    "v = collections.defaultdict(float)\n",
    "all_v = off_policy_mc_prediction(episodes, gamma)\n",
    "for state in states:\n",
    "    v[state] = all_v[state]\n",
    "print(dict(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (10 points)\n",
    "Modify the algorithm you developed in Problem 2 in order to estimate $v_\\pi(s)$ for $s\\in\\{s_{ABC},s_{AC},s_{C}\\}$ via Off-policy one-step TD prediction. Print the result for $\\gamma=0.99$ and $\\alpha=0.15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 1, 1): 41.01208121911871, (1, 0, 1): 27.7354244885284, (0, 0, 1): 13.897110376645768}\n"
     ]
    }
   ],
   "source": [
    "def one_step_td_prediction(alpha, gamma, episodes):\n",
    "    V = collections.defaultdict(float)\n",
    "    for episode in episodes:\n",
    "        state = [1, 1, 1]\n",
    "        for time_step in episode:\n",
    "            action = time_step[0]\n",
    "            reward = time_step[1]\n",
    "            next_state = state.copy()\n",
    "            next_state[action_index(action)] = 0\n",
    "            importance_sampling_ratio = get_target_prob(action, tuple(state))/get_behavior_prob(action, tuple(state))\n",
    "            V[tuple(state)] += alpha*importance_sampling_ratio*(reward+gamma*V[tuple(next_state)]-V[tuple(state)])\n",
    "            state = next_state\n",
    "    return V\n",
    "\n",
    "states = [(1, 1, 1), (1, 0, 1), (0, 0, 1)]\n",
    "gamma = 0.99\n",
    "alpha = 0.15\n",
    "v = collections.defaultdict(float)\n",
    "all_v = one_step_td_prediction(alpha, gamma, episodes)\n",
    "for state in states:\n",
    "    v[state] = all_v[state]\n",
    "print(dict(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
